# claude-code-actions-litellm-experiment
⚡ Claude Code Actions を LiteLLM プロキシで検証するための実験的レポジトリ - API ルーティング、パフォーマンス、互換性のテスト

## 概要

このリポジトリは **Claude Code Actions と LiteLLM プロキシの統合実験**を行うためのテスト環境です。

### 🎯 実験目的
- Claude Code Actions を非 Claude モデルで動作させる可能性の検証
- 各 LLM モデルの互換性と性能評価
- 実用的な統合パターンの発見

### 📋 実験内容
7つの異なる LLM モデルを LiteLLM プロキシ経由で Claude Code Actions に接続し、同一タスク（ワークフローのコードレビュー）を実行して比較評価を行いました。

## Claude Code Actions ワークフローについて
`.github/workflows/claude-code.yml` では以下の GitHub Actions ワークフローを定義しています：

### トリガー条件
- **Issue コメント**: `@claude` を含むコメントが作成された時
- **プルリクエストレビューコメント**: `@claude` を含むレビューコメントが作成された時  
- **Issue**: `@claude` を含む Issue が作成・割り当てされた時
- **プルリクエストレビュー**: `@claude` を含むレビューが提出された時

### 主な機能
1. **自動応答**: `@claude` メンションに対して Claude が自動的に応答
2. **コード編集**: `Edit`, `Replace`, `View` ツールを使用してコードの編集が可能
3. **GitHub 操作**: Issue や Pull Request の作成が可能
4. **セキュリティ**: 必要最小限の権限（contents, pull-requests, issues, id-token への write アクセス）

### 設定
- **API キー**: `ANTHROPIC_API_KEY` を GitHub Secrets で設定
- **モデル**: `MODEL` 変数で Claude モデルを指定
- **ベース URL**: `ANTHROPIC_BASE_URL` で API エンドポイントを設定（LiteLLM プロキシ用）
- **高速モデル**: `ANTHROPIC_SMALL_FAST_MODEL` で軽量タスク用のモデルを設定

## トラブルシューティング

### LiteLLM 統合で Issue コメントが空になる問題

**症状**: 
- Claude Code Action が実行されるが、Issue へのコメントが空になる
- ログには `output_tokens` が記録されているのに、実際のコメント内容が反映されない

**原因**:
LiteLLM プロキシのレスポンス形式が Claude Code Action の期待する形式と異なる場合があります。特に以下のケースで問題が発生します：
- 非 Claude モデル（Qwen、GPT など）を使用している場合
- LiteLLM のレスポンス形式が Anthropic API と完全互換でない場合

**診断方法**:
1. GitHub Actions のログで以下を確認：
   - `output_tokens` の値（0でないことを確認）
   - `claude-execution-output.json` の内容
2. デバッグステップが追加されている場合は、raw output を確認

**対処法**:
1. **モデルの確認**: Claude 互換モデルを使用しているか確認
2. **LiteLLM 設定の確認**: 
   ```yaml
   # LiteLLM config.yaml で streaming を無効化
   model_list:
     - model_name: your-model
       litellm_params:
         streaming: false
   ```
3. **API 形式の確認**: LiteLLM が Anthropic API 形式でレスポンスを返すよう設定
4. **デバッグログの確認**: ワークフローにデバッグステップが含まれている場合は、詳細なログを確認

**既知の制限事項**:
- 一部の LLM モデルでは、Claude Code Action のツール使用機能が正しく動作しない場合があります
- ストリーミングレスポンスは現在サポートされていません

## モデル評価結果

### 📊 モデル性能比較表

### 📊 モデル性能比較表

| モデル名 | 評価 | Issue | 実行状態 | 実行時間 | 主な問題 | 実用性 |
|---------|------|-------|---------|---------|---------|--------|
| **gemini-2.5-pro** | ⭐⭐⭐⭐⭐ (5/5) | #18 | ✅ 成功 | 46.2秒 | なし | **非常に実用的** |
| **gpt-5-mini** | ⭐⭐⭐⭐✨ (4.5/5) | #6 | ✅ 成功 | 115.6秒 | 内部思考の露出 | **最も実用的** |
| **glm-4.5** | ⭐⭐⭐⭐ (4/5) | #9 | ✅ 成功 | 57.1秒 | なし | **実用的** |
| **gpt-oss-120b** | ⭐⭐⭐ (3/5) | #5 | ✅ 成功 | 28.1秒 | 表面的な分析 | 実用可能 |
| **gpt-5-nano** | ⭐⭐ (2/5) | #8 | ⚠️ 部分的 | 296.4秒 | 57ターンの無限ループ | 重大な問題 |
| **qwen3-coder** | ⭐ (1/5) | #4 | ❌ 失敗 | - | 出力トークン0 | 使用不可 |
| **kimi-k2** | ⭐ (0/5) | #10 | ❌ 失敗 | - | ツール使用非対応 | 使用不可 |
| **gpt-5** | ⭐ (0/5) | #7 | ❌ 失敗 | - | タイムアウトエラー | 使用不可 |

### 💰 コスト分析

#### OpenRouter価格情報（2025年8月調査）

| モデル名 | 入力価格/1Mトークン | 出力価格/1Mトークン | コスト効率 | 特徴 |
|---------|------------------|------------------|----------|------|
| **gpt-5-nano** | $0.050 | $0.400 | 🔥 最安値 | 圧倒的なコストパフォーマンス |
| **gpt-oss-120b** | $0.100 | $0.500 | 🔥 高コスパ | 高速実行と低価格の両立 |
| **kimi-k2** | $0.140 | $2.490 | ⚠️ 出力高価 | 入力は安いが出力は高価 |
| **qwen3-coder** | $0.200 | $0.800 | ✅ バランス型 | 中価格帯の選択肢 |
| **glm-4.5** | $0.600 | $2.200 | ⚠️ 中価格 | 高性能だが価格も相応 |
| **gpt-5-mini** | $0.250 | $2.000 | ⚠️ 中価格 | 詳細分析に適した価格帯 |
| **gpt-5** | $1.250 | $10.000 | ❌ 最高価格 | 非常に高価なプレミアムモデル |
| **gemini-2.5-pro** | - | - | - | 最新鋭の高性能モデル |

**注**: 価格情報は変動する可能性があるため、最新の情報を公式サイトで確認してください。

#### 💡 コストパフォーマンス分析

**最もお得なモデル**:
1. **gpt-5-nano**: 圧倒的に安価（入力$0.05/1M、出力$0.40/1M）
   - ただし57ターンの無限ループ問題で実用性に課題
2. **gpt-oss-120b**: 速度とコストの最高バランス（入力$0.10/1M、出力$0.50/1M）
   - 28.1秒の高速実行と低価格を両立
   - **実用性を考慮すると最もコスパが良い**

**中価格帯モデル**:
- **glm-4.5**: 高性能だが価格上昇（入力$0.60/1M、出力$2.20/1M）
  - 以前の情報より価格が高い（実際はkimi-k2に近い価格帯）
- **gpt-5-mini**: バランス型（入力$0.25/1M、出力$2.00/1M）
- **qwen3-coder**: 中価格（入力$0.20/1M、出力$0.80/1M）だが動作しない

**価格注意点**:
- **kimi-k2**: 入力は安いが出力が非常に高価（$2.49/1M）、かつツール使用不可
- **gpt-5**: プレミアム価格（入力$1.25/1M、出力$10.00/1M）ながら実験では失敗

#### 💰 推奨モデル選択ガイド（価格×性能）

| 利用シーン | 推奨モデル | 理由 | コスト目安 |
|-----------|-----------|------|-----------|
| **最高コスパ** | gpt-oss-120b | 高速実行（28.1秒）と低価格（$0.10/$0.50）の両立 | 低 |
| **品質重視** | gpt-5-mini | 詳細な分析力、中価格（$0.25/$2.00） | 中 |
| **バランス型** | glm-4.5 | 高品質だが価格上昇（$0.60/$2.20） | 中〜高 |
| **避けるべき** | kimi-k2, gpt-5, qwen3-coder | 互換性問題または価格対効果が悪い | - |

#### 🎯 最終推奨（2025年8月版）

**実用的な選択肢（価格順）**:
1. **gpt-oss-120b** - コスパ最強、簡単〜中程度のタスクに最適
2. **gpt-5-mini** - 中価格で高品質、複雑なタスクに適している
3. **glm-4.5** - 高性能だが価格も高め、品質最優先の場合

**注意**: 価格情報は2025年8月時点のOpenRouter調査に基づく。GLM-4.5の価格は当初の予想より高く、慎重な選択が必要。

### 🔍 詳細評価

#### **gemini-2.5-pro** - 最高性能モデル
- **強み**:
  - issue の文脈を深く理解し、的確な改善提案を生成
  - 自然で高品質な日本語のドキュメントレビューを実施
  - 内部思考プロセスが露出せず、出力がクリーン
  - 1ターンでタスクを完結させる高い効率性
- **弱点**:
  - OpenRouterでの価格が未定（2025年8月時点）
- **推奨用途**: 高度なレビューやドキュメント生成など、品質が最優先されるタスク

#### **gpt-5-mini** - 最優秀モデル
- **強み**:
  - 実際のコードを読み込み、具体的な行番号付きでレビュー
  - セキュリティリスクを詳細に分析（モデル入力検証、権限管理、情報漏洩など）
  - 各問題に対して実用的な対策案を提示
  - 1ターンで効率的に完結
- **弱点**:
  - `<analysis>`タグで内部思考プロセスが露出
  - Claude Code Action の標準形式と若干異なる
- **推奨用途**: プロダクション環境での実用

#### **glm-4.5** - 優秀な新モデル
- **強み**:
  - 適切な構造化レビュー（タスクリスト形式）
  - 具体的な行番号付きの問題指摘
  - セキュリティ、コード重複、ハードコーディングなど多角的な分析
  - クリーンな出力形式（内部思考の露出なし）
  - 1ターンで完結し、効率的
- **弱点**:
  - 特になし（安定した実用的なパフォーマンス）
- **推奨用途**: プロダクション環境での実用

#### **gpt-oss-120b** - 安定動作モデル
- **強み**:
  - 簡潔で読みやすい出力
  - 基本的なベストプラクティスをカバー
  - 安定した動作
- **弱点**:
  - 具体的な行番号への参照なし
  - 実際のコード分析が不足
  - 汎用的な回答になりがち
- **推奨用途**: 簡単なタスクや概要レビュー

#### **gpt-5-nano** - 非効率モデル
- **強み**:
  - 深い分析能力
  - 構造化された思考プロセス
- **弱点**:
  - **57ターン**という異常な実行回数（通常1ターン）
  - 約5分の実行時間
  - タスクを自己完結できず無限ループ
  - 「次のアクションを指示ください」と繰り返す
- **推奨用途**: 使用非推奨

#### **qwen3-coder** - 互換性なし
- **問題点**:
  - `output_tokens: 0` で実質的に空のレスポンス
  - `"(no content)"` という出力
  - Claude Code Action との互換性なし
- **推奨用途**: 使用不可

#### **kimi-k2** - ツール使用非対応
- **問題点**:
  - **ツール使用（Function Calling）をサポートしていない**
  - OpenRouter エラー: `No endpoints found that support tool use`
  - Claude Code Action との構造的な非互換性
- **推奨用途**: 使用不可

#### **gpt-5** - 存在しないモデル
- **問題点**:
  - LiteLLM でタイムアウトエラー
  - モデル名が架空の可能性
  - `Error calling litellm.acompletion for non-Anthropic model`
- **推奨用途**: 使用不可

### ⚠️ 技術的な問題と対策

#### 1. レスポンス形式の不一致
**問題**: LiteLLM のレスポンス形式が Claude Code Action の期待と異なる
**対策**:
```yaml
# LiteLLM config.yaml
model_list:
  - model_name: your-model
    litellm_params:
      streaming: false  # ストリーミングを無効化
      timeout: 120      # タイムアウトを延長
```

#### 2. タスク完了判定の問題
**問題**: 一部モデル（gpt-5-nano）がタスクを完了できない
**対策**: Claude Code Action 側でターン数制限を設定

#### 3. 内部思考の露出
**問題**: `<analysis>`タグなど内部処理が表示される
**対策**: プロンプトエンジニアリングで制御

#### 4. ツール使用のサポート
**問題**: 一部モデル（kimi-k2）がツール使用をサポートしない
**対策**: モデル選定時にツール使用サポートを必須要件として確認

### 🎯 推奨事項

1. **プロダクション環境**: 
   - 第1選択: Claude モデル（claude-sonnet-4-20250514）
   - 第2選択: gpt-5-mini（LiteLLM 経由）
   - 第3選択: glm-4.5（LiteLLM 経由）

2. **開発・テスト環境**:
   - glm-4.5 または gpt-5-mini で複雑なタスクを検証
   - gpt-oss-120b で簡単なタスクをテスト

3. **避けるべきモデル**:
   - kimi-k2（ツール使用非対応）
   - qwen3-coder（互換性なし）
   - gpt-5-nano（無限ループ問題）
   - gpt-5（存在しない/タイムアウト）

### 📈 パフォーマンス指標

| モデル | ターン数 | 実測実行時間 | ワークフロー総時間 | 出力品質 | 安定性 |
|--------|---------|------------|---------------|---------|--------|
| gpt-oss-120b | 20 | 28.1秒 | 62秒 | 中 | 高 |
| glm-4.5 | 16 | 57.1秒 | 83秒 | 高 | 高 |
| gpt-5-mini | 21 | 115.6秒 | 144秒 | 高 | 高 |
| gpt-5-nano | 57 | 296.4秒 | 317秒 | 低 | 低 |
| qwen3-coder | - | - | 21秒 | なし | 失敗 |
| kimi-k2 | - | - | 231秒* | なし | 失敗 |
| gpt-5 | - | - | 512秒* | なし | 失敗 |

*キャンセルされたワークフロー

## 🔬 実験結論

### 主要な発見

1. **API 互換性の重要性**
   - Claude Code Action は Anthropic API の特定の形式を期待
   - 非 Claude モデルは追加の設定やアダプターが必要

2. **モデル特性の違い**
   - **gpt-5-mini**: 詳細な分析力と実用性のバランスが最良（115.6秒）
   - **glm-4.5**: クリーンな出力と高品質な分析を両立（57.1秒）
   - **gpt-oss-120b**: 最速だが分析が表面的（28.1秒）
   - **gpt-5-nano**: 過度に複雑な思考プロセスで実用性に問題（296.4秒）
   - **kimi-k2**: ツール使用機能の欠如で根本的に非互換

3. **技術的課題**
   - タスク完了判定のロジック
   - ストリーミングレスポンスの処理
   - エラーハンドリングとフォールバック
   - ツール使用（Function Calling）のサポート確認

4. **パフォーマンス特性**
   - 実行速度と品質はトレードオフの関係
   - ターン数が多いほど実行時間が長くなる（gpt-5-nano: 57ターン）
   - 最適なバランスは glm-4.5（16ターン、57.1秒）

### 実用的な教訓

- **本番環境では Claude モデルを優先**すべき
- LiteLLM 統合は**開発・テスト用途**に適している
- モデル選択時は**タスクの複雑さ**に応じて判断
- **デバッグ機能の重要性**（今回の実験で追加）
- **ツール使用サポートの事前確認**が必須
- **glm-4.5 と gpt-5-mini が非 Claude モデルの中で最も実用的**
- **実行速度重視なら gpt-oss-120b（28.1秒）、品質重視なら glm-4.5（57.1秒）**

## 🚀 今後の展望

### 改善提案

1. **ワークフローの最適化**
   - モデル別のタイムアウト設定
   - ターン数制限の実装
   - より詳細なエラーレポート

2. **互換性レイヤーの開発**
   - 各モデル用のアダプター作成
   - レスポンス形式の正規化
   - プロンプトの最適化

3. **モニタリングの強化**
   - 実行時間の記録
   - 成功率の追跡
   - コスト分析

### 継続的な検証

このリポジトリは LiteLLM と Claude Code Actions の統合における**リファレンス実装**として機能します。新しいモデルや設定の検証に活用してください。

## 📚 関連リソース

- [Claude Code Actions Documentation](https://github.com/anthropics/claude-code-action)
- [LiteLLM Documentation](https://docs.litellm.ai/)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

---

**最終更新**: 2025年8月8日
**実験実施期間**: Issues #4-#10
**テストモデル数**: 7モデル
**テスト環境**: LiteLLM プロキシ経由での各種 LLM モデル
