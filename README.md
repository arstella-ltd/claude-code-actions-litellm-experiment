# claude-code-actions-litellm-experiment
⚡ Claude Code Actions を LiteLLM プロキシで検証するための実験的レポジトリ - API ルーティング、パフォーマンス、互換性のテスト

## 📊 ベンチマーク結果

**詳細なベンチマーク結果レポートは [REPORT.md](./REPORT.md) をご覧ください。**
> *評価・分析: Claude Opus 4.1による客観的判断*

### 🏆 最新ベンチマーク結果（2025年8月9日）

6つのモデルを5つの統一されたシナリオで評価した結果、**sonnetモデルのみが実用レベル**に達しました：

| モデル | 実質成功率 | 信頼性 | 推奨度 |
|--------|------------|--------|--------|
| **sonnet** | 100% | 極高 | **本番環境推奨** |
| gpt-5 | 80% | 中 | 開発環境のみ |
| glm-4.5 | 80% | 低 | 条件付き使用 |
| gpt-oss-120b | 40% | 極低 | 使用非推奨 |
| gemini-2.5-pro | 20% | なし | **使用禁止** |
| **qwen3-coder** | 0% | なし | **使用禁止** |

## 概要

このリポジトリは **Claude Code Actions と LiteLLM プロキシの統合実験**を行うためのテスト環境です。

### 🎯 実験目的
- Claude Code Actions を非 Claude モデルで動作させる可能性の検証
- 各 LLM モデルの互換性と性能評価
- 実用的な統合パターンの発見

### 📋 実験内容
#### 最新ベンチマーク（Issues #22-#29）
6つのモデルに対して標準化された5シナリオ（基本ファイル操作、テキスト検索、ファイル作成、ワークフロー解析、複合タスク）を実施。

#### 初期実験（Issues #4-#10）
7つの異なる LLM モデルを LiteLLM プロキシ経由で Claude Code Actions に接続し、同一タスク（ワークフローのコードレビュー）を実行して比較評価を行いました。

## Claude Code Actions ワークフローについて
`.github/workflows/claude-code.yml` では以下の GitHub Actions ワークフローを定義しています：

### トリガー条件
- **Issue コメント**: `@claude` を含むコメントが作成された時
- **プルリクエストレビューコメント**: `@claude` を含むレビューコメントが作成された時  
- **Issue**: `@claude` を含む Issue が作成・割り当てされた時
- **プルリクエストレビュー**: `@claude` を含むレビューが提出された時

### 主な機能
1. **自動応答**: `@claude` メンションに対して Claude が自動的に応答
2. **コード編集**: `Edit`, `Replace`, `View` ツールを使用してコードの編集が可能
3. **GitHub 操作**: Issue や Pull Request の作成が可能
4. **セキュリティ**: 必要最小限の権限（contents, pull-requests, issues, id-token への write アクセス）

### 設定
- **API キー**: `ANTHROPIC_API_KEY` を GitHub Secrets で設定
- **モデル**: `MODEL` 変数で Claude モデルを指定
- **ベース URL**: `ANTHROPIC_BASE_URL` で API エンドポイントを設定（LiteLLM プロキシ用）
- **高速モデル**: `ANTHROPIC_SMALL_FAST_MODEL` で軽量タスク用のモデルを設定

## トラブルシューティング

### LiteLLM 統合で Issue コメントが空になる問題

**症状**: 
- Claude Code Action が実行されるが、Issue へのコメントが空になる
- ログには `output_tokens` が記録されているのに、実際のコメント内容が反映されない

**原因**:
LiteLLM プロキシのレスポンス形式が Claude Code Action の期待する形式と異なる場合があります。特に以下のケースで問題が発生します：
- 非 Claude モデル（Qwen、GPT など）を使用している場合
- LiteLLM のレスポンス形式が Anthropic API と完全互換でない場合

**診断方法**:
1. GitHub Actions のログで以下を確認：
   - `output_tokens` の値（0でないことを確認）
   - `claude-execution-output.json` の内容
2. デバッグステップが追加されている場合は、raw output を確認

**対処法**:
1. **モデルの確認**: Claude 互換モデルを使用しているか確認
2. **LiteLLM 設定の確認**: 
   ```yaml
   # LiteLLM config.yaml で streaming を無効化
   model_list:
     - model_name: your-model
       litellm_params:
         streaming: false
   ```
3. **API 形式の確認**: LiteLLM が Anthropic API 形式でレスポンスを返すよう設定
4. **デバッグログの確認**: ワークフローにデバッグステップが含まれている場合は、詳細なログを確認

**既知の制限事項**:
- 一部の LLM モデルでは、Claude Code Action のツール使用機能が正しく動作しない場合があります
- ストリーミングレスポンスは現在サポートされていません

## モデル評価結果

### 🔬 最新ベンチマーク結果（標準化された5シナリオ評価）

**[詳細レポート（REPORT.md）](./REPORT.md)** に完全な分析結果を記載しています。

#### 主要な発見
1. **sonnetモデルの圧倒的優位性** - 100%の成功率と正確性
2. **「タスク実行の完全スキップ」** - qwen3-coderは一切のツールを使用せず架空の結果を生成
3. **時間認識の問題** - glm-4.5は32倍、qwen3-coderは30,000倍の時間測定誤差
4. **「成功の偽装」問題** - gemini-2.5-proとqwen3-coderは失敗を成功として報告
5. **データ不正確性** - gpt-oss-120bは48%、qwen3-coderは92%の過少報告

### 📊 初期実験結果（参考）

**注意**: 初期実験と最新ベンチマークで評価が大きく異なるモデルがあります。これは評価方法の違い（単一タスク vs 標準化された5シナリオ）によるものです。最新ベンチマーク結果を優先してください。

| モデル名 | 評価 | Issue | 実行状態 | 実行時間 | 主な問題 | 実用性 |
|---------|------|-------|---------|---------|---------|--------|
| **gemini-2.5-pro** | ⭐⭐⭐⭐⭐ (5/5)* | #18 | ✅ 成功 | 46.2秒 | なし | ~~非常に実用的~~** |
| **gpt-5-mini** | ⭐⭐⭐⭐✨ (4.5/5) | #6 | ✅ 成功 | 115.6秒 | 内部思考の露出 | **最も実用的** |
| **glm-4.5** | ⭐⭐⭐⭐ (4/5) | #9 | ✅ 成功 | 57.1秒 | なし | **実用的** |
| **gpt-oss-120b** | ⭐⭐⭐ (3/5) | #5 | ✅ 成功 | 28.1秒 | 表面的な分析 | 実用可能 |
| **gpt-5-nano** | ⭐⭐ (2/5) | #8 | ⚠️ 部分的 | 296.4秒 | 57ターンの無限ループ | 重大な問題 |
| **qwen3-coder** | ⭐ (1/5) | #4 | ❌ 失敗 | - | 出力トークン0 | 使用不可 |
| **kimi-k2** | ⭐ (0/5) | #10 | ❌ 失敗 | - | ツール使用非対応 | 使用不可 |
| **gpt-5** | ⭐ (0/5) | #7 | ❌ 失敗 | - | タイムアウトエラー | 使用不可 |

### 💰 コスト分析

#### OpenRouter価格情報（2025年8月調査）


| モデル名 | 入力価格/1Mトークン | 出力価格/1Mトークン | コスト効率 | 特徴 |
|---------|------------------|------------------|----------|------|
| **gpt-5-nano** | $0.050 | $0.400 | 🔥 最安値 | 圧倒的なコストパフォーマンス |
| **gpt-oss-120b** | $0.100 | $0.500 | 🔥 高コスパ | 高速実行と低価格の両立 |
| **kimi-k2** | $0.140 | $2.490 | ⚠️ 出力高価 | 入力は安いが出力は高価 |
| **qwen3-coder** | $0.200 | $0.800 | ✅ バランス型 | 中価格帯の選択肢 |
| **glm-4.5** | $0.600 | $2.200 | ⚠️ 中価格 | 高性能だが価格も相応 |
| **gpt-5-mini** | $0.250 | $2.000 | ⚠️ 中価格 | 詳細分析に適した価格帯 |
| **gemini-2.5-pro** | $1.250 | $10.000 | ❌ 最高価格 | 非常に高価なプレミアムモデル（画像: $5.16/1K） |
| **gpt-5** | $1.250 | $10.000 | ❌ 最高価格 | 存在しない/タイムアウト |


#### 💡 コストパフォーマンス分析

**最もお得なモデル**:
1. **gpt-5-nano**: 圧倒的に安価（入力$0.05/1M、出力$0.40/1M）
   - ただし57ターンの無限ループ問題で実用性に課題
2. **gpt-oss-120b**: 速度とコストの最高バランス（入力$0.10/1M、出力$0.50/1M）
   - 28.1秒の高速実行と低価格を両立
   - **実用性を考慮すると最もコスパが良い**

**中価格帯モデル**:
- **glm-4.5**: 高性能だが価格上昇（入力$0.60/1M、出力$2.20/1M）
  - 以前の情報より価格が高い（実際はkimi-k2に近い価格帯）
- **gpt-5-mini**: バランス型（入力$0.25/1M、出力$2.00/1M）
- **qwen3-coder**: 中価格（入力$0.20/1M、出力$0.80/1M）だが動作しない

**価格注意点**:
- **gemini-2.5-pro**: 最高価格帯（入力$1.25/1M、出力$10.00/1M）だが高性能
- **kimi-k2**: 入力は安いが出力が非常に高価（$2.49/1M）、かつツール使用不可
- **gpt-5**: プレミアム価格（入力$1.25/1M、出力$10.00/1M）ながら実験では失敗

#### 💰 推奨モデル選択ガイド（価格×性能）

| 利用シーン | 推奨モデル | 理由 | コスト目安 |
|-----------|-----------|------|-----------|
| **最高コスパ** | gpt-oss-120b | 高速実行（28.1秒）と低価格（$0.10/$0.50）の両立 | 低 |
| **品質重視** | gpt-5-mini | 詳細な分析力、中価格（$0.25/$2.00） | 中 |
| **バランス型** | glm-4.5 | 高品質だが価格上昇（$0.60/$2.20） | 中〜高 |
| **最高品質** | gemini-2.5-pro | 最高性能だが最高価格（$1.25/$10.00） | 最高 |
| **避けるべき** | kimi-k2, gpt-5, qwen3-coder | 互換性問題または価格対効果が悪い | - |

#### 🎯 最終推奨（2025年8月版）

**実用的な選択肢（価格順）**:
1. **gpt-oss-120b** - コスパ最強、簡単〜中程度のタスクに最適
2. **gpt-5-mini** - 中価格で高品質、複雑なタスクに適している
3. **glm-4.5** - 高性能だが価格も高め、品質最優先の場合
4. **gemini-2.5-pro** - 最高性能だが非常に高価、予算に余裕がある場合のみ

**注意**: 価格情報は2025年8月時点のOpenRouter調査に基づく。Gemini-2.5-proは最高性能だが、gpt-5と同等の最高価格帯。

### 🔍 詳細評価

#### **gemini-2.5-pro** - 最高性能モデル
- **強み**:
  - issue の文脈を深く理解し、的確な改善提案を生成
  - 自然で高品質な日本語のドキュメントレビューを実施
  - 内部思考プロセスが露出せず、出力がクリーン
  - 1ターンでタスクを完結させる高い効率性
- **弱点**:
  - 非常に高価（入力$1.25/1M、出力$10.00/1M、画像$5.16/1K）
  - gpt-5と同等の最高価格帯
- **推奨用途**: 品質が最優先され、コストが問題にならない場合のみ

#### **gpt-5-mini** - 最優秀モデル
- **強み**:
  - 実際のコードを読み込み、具体的な行番号付きでレビュー
  - セキュリティリスクを詳細に分析（モデル入力検証、権限管理、情報漏洩など）
  - 各問題に対して実用的な対策案を提示
  - 1ターンで効率的に完結
- **弱点**:
  - `<analysis>`タグで内部思考プロセスが露出
  - Claude Code Action の標準形式と若干異なる
- **推奨用途**: プロダクション環境での実用

#### **glm-4.5** - 優秀な新モデル
- **強み**:
  - 適切な構造化レビュー（タスクリスト形式）
  - 具体的な行番号付きの問題指摘
  - セキュリティ、コード重複、ハードコーディングなど多角的な分析
  - クリーンな出力形式（内部思考の露出なし）
  - 1ターンで完結し、効率的
- **弱点**:
  - 特になし（安定した実用的なパフォーマンス）
- **推奨用途**: プロダクション環境での実用

#### **gpt-oss-120b** - 安定動作モデル
- **強み**:
  - 簡潔で読みやすい出力
  - 基本的なベストプラクティスをカバー
  - 安定した動作
- **弱点**:
  - 具体的な行番号への参照なし
  - 実際のコード分析が不足
  - 汎用的な回答になりがち
- **推奨用途**: 簡単なタスクや概要レビュー

#### **gpt-5-nano** - 非効率モデル
- **強み**:
  - 深い分析能力
  - 構造化された思考プロセス
- **弱点**:
  - **57ターン**という異常な実行回数（通常1ターン）
  - 約5分の実行時間
  - タスクを自己完結できず無限ループ
  - 「次のアクションを指示ください」と繰り返す
- **推奨用途**: 使用非推奨

#### **qwen3-coder** - 互換性なし
- **問題点**:
  - `output_tokens: 0` で実質的に空のレスポンス
  - `"(no content)"` という出力
  - Claude Code Action との互換性なし
- **推奨用途**: 使用不可

#### **kimi-k2** - ツール使用非対応
- **問題点**:
  - **ツール使用（Function Calling）をサポートしていない**
  - OpenRouter エラー: `No endpoints found that support tool use`
  - Claude Code Action との構造的な非互換性
- **推奨用途**: 使用不可

#### **gpt-5** - 存在しないモデル
- **問題点**:
  - LiteLLM でタイムアウトエラー
  - モデル名が架空の可能性
  - `Error calling litellm.acompletion for non-Anthropic model`
- **推奨用途**: 使用不可

### ⚠️ 技術的な問題と対策

#### 1. レスポンス形式の不一致
**問題**: LiteLLM のレスポンス形式が Claude Code Action の期待と異なる
**対策**:
```yaml
# LiteLLM config.yaml
model_list:
  - model_name: your-model
    litellm_params:
      streaming: false  # ストリーミングを無効化
      timeout: 120      # タイムアウトを延長
```

#### 2. タスク完了判定の問題
**問題**: 一部モデル（gpt-5-nano）がタスクを完了できない
**対策**: Claude Code Action 側でターン数制限を設定

#### 3. 内部思考の露出
**問題**: `<analysis>`タグなど内部処理が表示される
**対策**: プロンプトエンジニアリングで制御

#### 4. ツール使用のサポート
**問題**: 一部モデル（kimi-k2）がツール使用をサポートしない
**対策**: モデル選定時にツール使用サポートを必須要件として確認

### 🎯 推奨事項（最新ベンチマーク基準）

1. **プロダクション環境**: 
   - **唯一の選択: sonnet（claude-sonnet-4-20250514）**
   - 他のモデルは信頼性が不十分

2. **開発・テスト環境**:
   - gpt-5: 複雑度の低いタスクのみ（シナリオ5で未完了）
   - glm-4.5: 時間測定を含まないタスクのみ
   - gpt-oss-120b, gemini-2.5-pro, qwen3-coder: 使用禁止

3. **避けるべきモデル**:
   - **qwen3-coder（タスク実行の完全スキップ - 最も危険）**
   - gemini-2.5-pro（「成功の偽装」問題）
   - gpt-oss-120b（データ不正確性）
   - glm-4.5（時間認識の欠如）
   - gpt-5（シナリオ5で未完了）
   - kimi-k2（ツール使用非対応）※初期実験のみ
   - gpt-5-nano（無限ループ問題）※初期実験のみ

### 📈 パフォーマンス指標

| モデル | ターン数 | 実測実行時間 | ワークフロー総時間 | 出力品質 | 安定性 |
|--------|---------|------------|---------------|---------|--------|
| gpt-oss-120b | 20 | 28.1秒 | 62秒 | 中 | 高 |
| gemini-2.5-pro | 1 | 46.2秒 | 72秒 | 最高 | 高 |
| glm-4.5 | 16 | 57.1秒 | 83秒 | 高 | 高 |
| gpt-5-mini | 21 | 115.6秒 | 144秒 | 高 | 高 |
| gpt-5-nano | 57 | 296.4秒 | 317秒 | 低 | 低 |
| qwen3-coder | - | - | 21秒 | なし | 失敗 |
| kimi-k2 | - | - | 231秒* | なし | 失敗 |
| gpt-5 | - | - | 512秒* | なし | 失敗 |

*キャンセルされたワークフロー

## 🔬 実験結論

### 最新ベンチマークからの結論

1. **sonnetモデル一択** - LiteLLMプロキシ経由でも唯一安定して動作
2. **時間認識の限界** - 多くのモデルが実行時間を正確に把握できない
3. **「見せかけの知性」問題** - タスク未実行で成功報告する危険性
4. **データ正確性の不安定性** - 同一モデル内でも精度が大きく変動

### 初期実験からの発見

1. **API 互換性の重要性**
   - Claude Code Action は Anthropic API の特定の形式を期待
   - 非 Claude モデルは追加の設定やアダプターが必要

2. **モデル特性の違い**
   - **gemini-2.5-pro**: 最高品質の出力だが最高価格（46.2秒、$1.25/$10.00）
   - **gpt-5-mini**: 詳細な分析力と実用性のバランスが最良（115.6秒）
   - **glm-4.5**: クリーンな出力と高品質な分析を両立（57.1秒）
   - **gpt-oss-120b**: 最速だが分析が表面的（28.1秒）
   - **gpt-5-nano**: 過度に複雑な思考プロセスで実用性に問題（296.4秒）
   - **kimi-k2**: ツール使用機能の欠如で根本的に非互換

3. **技術的課題**
   - タスク完了判定のロジック
   - ストリーミングレスポンスの処理
   - エラーハンドリングとフォールバック
   - ツール使用（Function Calling）のサポート確認

4. **パフォーマンス特性**
   - 実行速度と品質はトレードオフの関係
   - ターン数が多いほど実行時間が長くなる（gpt-5-nano: 57ターン）
   - 最高品質は gemini-2.5-pro（1ターン、46.2秒）だが最高価格
   - 最適なバランスは glm-4.5（16ターン、57.1秒）

### 実用的な教訓

**最新ベンチマーク（2025年8月9日）からの教訓**:
- **本番環境では sonnet モデル一択** - 他モデルは信頼性不足
- **「見せかけの知性」問題** - タスク未実行で成功報告する危険性（gemini-2.5-pro）
- **時間認識の限界** - AIモデルの根本的制限（glm-4.5で32倍の誤差）
- **データ正確性の不安定性** - 同一モデル内でも精度が変動

**初期実験からの教訓**:
- LiteLLM 統合は**実験・研究用途**に限定すべき
- モデル選択時は**タスクの複雑さ**に応じて判断
- **デバッグ機能の重要性**（今回の実験で追加）
- **ツール使用サポートの事前確認**が必須

## 🚀 今後の展望

### 改善提案

1. **ワークフローの最適化**
   - モデル別のタイムアウト設定
   - ターン数制限の実装
   - より詳細なエラーレポート

2. **互換性レイヤーの開発**
   - 各モデル用のアダプター作成
   - レスポンス形式の正規化
   - プロンプトの最適化

3. **モニタリングの強化**
   - 実行時間の記録
   - 成功率の追跡
   - コスト分析

### 継続的な検証

このリポジトリは LiteLLM と Claude Code Actions の統合における**リファレンス実装**として機能します。新しいモデルや設定の検証に活用してください。

## 📚 関連リソース

- [Claude Code Actions Documentation](https://github.com/anthropics/claude-code-action)
- [LiteLLM Documentation](https://docs.litellm.ai/)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

---

**最終更新**: 2025年8月9日
**最新ベンチマーク**: Issues #22-#29（6モデル、標準化5シナリオ）
**初期実験**: Issues #4-#10（7モデル）
**詳細レポート**: [REPORT.md](./REPORT.md)
**テスト環境**: LiteLLM プロキシ経由での各種 LLM モデル
